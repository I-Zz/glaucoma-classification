{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7860205,"sourceType":"datasetVersion","datasetId":4610776},{"sourceId":9781986,"sourceType":"datasetVersion","datasetId":5992903},{"sourceId":9788654,"sourceType":"datasetVersion","datasetId":5964505},{"sourceId":153165,"sourceType":"modelInstanceVersion","modelInstanceId":130089,"modelId":152943},{"sourceId":159881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":135926,"modelId":158650}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:36:52.299781Z","iopub.execute_input":"2024-11-08T11:36:52.300232Z","iopub.status.idle":"2024-11-08T11:37:10.070391Z","shell.execute_reply.started":"2024-11-08T11:36:52.300188Z","shell.execute_reply":"2024-11-08T11:37:10.068665Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.28-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (10.3.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.14.1)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.4.0+cpu)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.19.0+cpu)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.11-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.3.28-py3-none-any.whl (881 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.11-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.3.28 ultralytics-thop-2.0.11\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_and_predict_image(image, model_path, device):\n    \"\"\"\n    Load an image, apply transformations, and predict using a pretrained model.\n    \n    Parameters:\n    - img_path (str): Path to the image file.\n    - model_path (str): Path to the pretrained model file.\n    - device (torch.device): The device (GPU/CPU) to use for computation.\n    \n    Returns:\n    - float: The predicted probability of the image having Glaucoma.\n    - int: The binary classification result (1 for Glaucoma, 0 for No Glaucoma).\n    \"\"\"\n    # Define transformation for the image\n    def test_transform():\n        return transforms.Compose([\n            transforms.Resize((384, 384)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    # Load and transform the image\n#     image = Image.open(img_path).convert('RGB')\n    image = test_transform()(image).unsqueeze(0)  # Add batch dimension\n\n    # Load the model\n    model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n    num_features = model.heads.head.in_features\n    model.heads.head = nn.Linear(num_features, 1)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n\n    # Predict the image\n    with torch.no_grad():\n        image = image.to(device)\n        outputs = model(image)\n        probs = torch.sigmoid(outputs).squeeze().item()  # Squeeze to remove batch dimensions and get the scalar probability\n\n    # Determine binary classification\n    binary_pred = int(probs > 0.5)\n\n    return probs, binary_pred","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:39:14.746645Z","iopub.execute_input":"2024-11-08T11:39:14.747232Z","iopub.status.idle":"2024-11-08T11:39:14.763218Z","shell.execute_reply.started":"2024-11-08T11:39:14.747183Z","shell.execute_reply":"2024-11-08T11:39:14.761643Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\n\n# Define the transformation pipeline\npreprocess = transforms.Compose([\n    transforms.Resize((384, 384), interpolation=transforms.InterpolationMode.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef apply_clahe(img, clip_limit=3.0, tile_grid_size=(8, 8)):\n    \"\"\"Apply CLAHE contrast enhancement on each color channel separately.\"\"\"\n    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n    r, g, b = cv2.split(img_cv)\n    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n    r_clahe = clahe.apply(r)\n    g_clahe = clahe.apply(g)\n    b_clahe = clahe.apply(b)\n    clahe_img_cv = cv2.merge([r_clahe, g_clahe, b_clahe])\n    clahe_img = cv2.cvtColor(clahe_img_cv, cv2.COLOR_BGR2RGB)\n    return Image.fromarray(clahe_img)\n\ndef trim_and_resize(im, output_size):\n    \"\"\"Trim margins, maintain aspect ratio, and resize to the specified output size.\"\"\"\n    percentage = 0.02\n    img = np.array(im)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    im_binary = img_gray > 0.1 * np.mean(img_gray[img_gray != 0])\n    row_sums = np.sum(im_binary, axis=1)\n    col_sums = np.sum(im_binary, axis=0)\n    rows = np.where(row_sums > img.shape[1] * percentage)[0]\n    cols = np.where(col_sums > img.shape[0] * percentage)[0]\n    if rows.size and cols.size:\n        min_row, min_col = np.min(rows), np.min(cols)\n        max_row, max_col = np.max(rows), np.max(cols)\n        img = img[min_row:max_row+1, min_col:max_col+1]\n    im_pil = Image.fromarray(img)\n    old_size = im_pil.size\n    ratio = float(output_size) / max(old_size)\n    new_size = tuple([int(x * ratio) for x in old_size])\n    im_resized = im_pil.resize(new_size, Image.LANCZOS)\n    new_im = Image.new(\"RGB\", (output_size, output_size))\n    new_im.paste(im_resized, ((output_size - new_size[0]) // 2, (output_size - new_size[1]) // 2))\n    return new_im\n\n# Load the YOLO model\nyolo_model = YOLO('/kaggle/input/od_segmentation_model/pytorch/default/1/best.pt')\n\n# Load the ViT model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# vit_model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1)\n# num_features = vit_model.heads.head.in_features\n# vit_model.heads.head = nn.Linear(num_features, 1)  # Assuming binary classification\nbest_model_path = '/kaggle/input/new_vit_roi_best_model/pytorch/default/1/ViT_RG_ROI_best (1).pth'\n# vit_model.load_state_dict(torch.load(best_model_path, map_location=device))\n# vit_model.to(device)\n# vit_model.eval()\n\ndef find_encompassing_bbox(bboxes):\n    \"\"\"Find the smallest bounding box that encompasses all given bounding boxes.\"\"\"\n    if not bboxes:\n        return None\n    x_min = min(box[0] for box in bboxes)\n    y_min = min(box[1] for box in bboxes)\n    x_max = max(box[2] for box in bboxes)\n    y_max = max(box[3] for box in bboxes)\n    return [x_min, y_min, x_max, y_max]\n\n\ndef crop_and_resize_image(img, bbox, target_size=(518, 518)):\n    \"\"\"\n    Attempts to crop a 518x518 square from the center of a given bounding box. If the square\n    exceeds image dimensions, it crops the largest possible square and resizes it to 518x518.\n    If a given bounding box exceeds 518x518, segmentation is considered as invalid.\n\n    Parameters:\n    - img: The preprocessed image read by CV2.\n    - bbox: The bounding box coordinates as [x1, y1, x2, y2].\n    - target_size: The target size for cropping and resizing as (width, height).\n\n    Returns:\n    - Resized image if cropping is possible, otherwise None.\n    \"\"\"\n    x1, y1, x2, y2 = map(int, bbox)\n    \n    # Calculate the width and height of the bounding box\n    bbox_width = x2 - x1\n    bbox_height = y2 - y1\n\n    # Check if the bounding box doesn't exceed 518x518\n    if bbox_width > target_size[0] or bbox_height > target_size[1]:\n        return None \n\n    # Calculate the center of the bounding box\n    center_x = (x1 + x2) // 2\n    center_y = (y1 + y2) // 2\n\n    # Calculate half the target size\n    half_target = target_size[0] // 2\n\n    # Define initial maximum square crop that can fit within the image boundaries\n    start_x = max(0, center_x - half_target)\n    start_y = max(0, center_y - half_target)\n    end_x = min(img.shape[1], center_x + half_target)\n    end_y = min(img.shape[0], center_y + half_target)\n\n    # Validate crop dimensions\n    crop_width = end_x - start_x\n    crop_height = end_y - start_y\n\n    # Adjust crop dimensions to the largest possible square within the boundary\n    if crop_width < target_size[0] or crop_height < target_size[1]:\n        # Calculate the largest possible dimension that can be squared within the limits\n        max_possible_square = min(crop_width, crop_height)\n        start_x = center_x - max_possible_square // 2\n        start_y = center_y - max_possible_square // 2\n        end_x = start_x + max_possible_square\n        end_y = start_y + max_possible_square\n        # Re-validate boundaries (important in cases where center is near the image edge)\n        start_x = max(0, start_x)\n        start_y = max(0, start_y)\n        end_x = min(img.shape[1], end_x)\n        end_y = min(img.shape[0], end_y)\n\n    # Crop the image\n    cropped_img = img[start_y:end_y, start_x:end_x]\n    if cropped_img.size == 0:\n        return None  # Return None if the cropped image is empty\n\n    # Resize to the desired target size\n    resized_img = cv2.resize(cropped_img, target_size, interpolation=cv2.INTER_CUBIC)\n\n    return resized_img\n\ndef inference(img_path):\n    \"\"\"\n    Performs inference on the given image.\n\n    Parameters:\n    - img_path (str): Path to the input image.\n\n    Returns:\n    - The predicted label (0 or 1) and the prediction probability.\n    \"\"\"\n    try:\n        # Load and preprocess the image\n        img = Image.open(img_path).convert(\"RGB\")\n        output_size = 2000\n        img = trim_and_resize(img, output_size)  # Resize to 384 to match the transform\n        img_cl = apply_clahe(img)\n        img_array = np.array(img_cl)\n        if len(img_array.shape) == 3 and img_array.shape[2] == 3:  # If RGB\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n#         img_array = cv2.imread(img_cl)\n        \n        # Detect ROI using YOLO\n        results = yolo_model(img_array)\n        boxes = results[0].boxes\n        if len(boxes) == 0:\n            print(\"ROI not detected in the image.\")\n            return None, None\n\n        # Find the encompassing bounding box\n        bboxes = [box.cpu().numpy().tolist() for box in boxes.xyxy]\n        encompassing_bbox = find_encompassing_bbox(bboxes)\n        if encompassing_bbox is None:\n            print(\"No bounding boxes found.\")\n            return None, None\n\n        # Crop and resize the image\n        cropped_resized_img = crop_and_resize_image(img_array, encompassing_bbox, target_size=(518, 518))\n#         ropped_resized_img = crop_and_resize_image(img_array, encompassing_bbox, target_size=size)\n        if cropped_resized_img is None:\n            print(\"Bounding box was too large to crop.\")\n            return None, None\n\n        # Convert to PIL Image for transformations\n        cropped_pil = Image.fromarray(cropped_resized_img)\n        probs,binary_pred= load_and_predict_image(cropped_pil,best_model_path,device)\n\n        return binary_pred,probs\n\n    except Exception as e:\n        print(f\"An error occurred during inference: {e}\")\n        return None, None\n\n# Example usage\nimg_path = '/kaggle/input/jraigs-dataset/justRAIGS/1/TRAIN024379.JPG'\npred, prob = inference(img_path)\nif pred is not None:\n    if pred==1:\n        print(f\"Predicted label: Referable glaucoma\")\n    if pred==0:\n        print(f\"Predicted label: Non Referable glaucoma\")\n        \n    print(f\"Prediction probability: {prob:.4f}\")\nelse:\n    print(\"Inference could not be completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T11:41:40.077600Z","iopub.execute_input":"2024-11-08T11:41:40.078291Z","iopub.status.idle":"2024-11-08T11:41:44.085175Z","shell.execute_reply.started":"2024-11-08T11:41:40.078246Z","shell.execute_reply":"2024-11-08T11:41:44.082777Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n0: 640x640 2 disks, 149.7ms\nSpeed: 5.6ms preprocess, 149.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\nPredicted label: Referable glaucoma\nPrediction probability: 0.9067\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}